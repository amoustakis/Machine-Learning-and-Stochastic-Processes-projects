{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["**Απόστολος Μουστάκης**\n","\n","**AM: 03400182**"],"metadata":{"id":"05j5tERMszYQ"}},{"cell_type":"markdown","source":["<font color='darkred'>Οι απαντήσεις σε όλα τα ερωτήματα γράφονται με αυτό το χρώμα ώστε να ξεχωρίζουν από τις εκφωνήσεις </font>"],"metadata":{"id":"piMI1RxWs0en"}},{"cell_type":"markdown","metadata":{"id":"XLZdEbAy2jfr"},"source":["<h1><b>Markov Decision Processes</h1></b>\n","<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n","<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n"]},{"cell_type":"markdown","metadata":{"id":"6VsUV229__zO"},"source":["<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>"]},{"cell_type":"code","metadata":{"id":"OM8ivgOJAg_H"},"source":["!pip uninstall -y gym\n","!pip install setuptools==66\n","!pip install gym==0.18.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import gym\n","from gym import wrappers"],"metadata":{"id":"MJmIcwKPrKBk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_puV3ugeAnbU"},"source":["Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:"]},{"cell_type":"code","metadata":{"id":"Ep-MvIUCAxT8"},"source":["env_name = 'FrozenLake8x8-v0'\n","env = gym.make(env_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uBKBXJDUBRUh"},"source":["Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent"]},{"cell_type":"code","metadata":{"id":"p6lqbG9zBgdi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eaa948c3-5510-4c16-cfc3-b17bc6cc9101","executionInfo":{"status":"ok","timestamp":1682598337102,"user_tz":-180,"elapsed":262,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}}},"source":["env.reset()\n","env.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[41mS\u001b[0mFFFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n"]}]},{"cell_type":"markdown","metadata":{"id":"FX2res4JBlYb"},"source":["Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα."]},{"cell_type":"code","metadata":{"id":"Gq7q944YBx0Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e8ac5dc3-b032-4269-a6bb-1e5600fe2038","executionInfo":{"status":"ok","timestamp":1682598340305,"user_tz":-180,"elapsed":255,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}}},"source":["next_action = env.action_space.sample()\n","env.step(next_action)\n","env.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  (Left)\n","\u001b[41mS\u001b[0mFFFFFFF\n","FFFFFFFF\n","FFFHFFFF\n","FFFFFHFF\n","FFFHFFFF\n","FHHFFFHF\n","FHFFHFHF\n","FFFHFFFG\n"]}]},{"cell_type":"markdown","metadata":{"id":"mV4A7lsLB54y"},"source":["Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;"]},{"cell_type":"markdown","source":["<font color='darkred'>Εκτελώντας τις τελευταίες εντολές αρκετές φορές παρατηρώ πως ο πράκτορας (agent) δεν εκτελεί πάντα την κίνηση που του ζητείται. Συνεπώς οι κινήσεις του είναι στοχαστικές και όχι ντετερμινιστικές. Ο λόγος που συμβαίνει αυτό είναι διότι υπάρχουν δύο εκδόσεις του προβλήματος της Παγωμένης Λίμνης (Frozen Lake). Η μία έκδοση έχει ολισθηρό πάγο (slippery ice), όπου οι επιλεγμένες ενέργειες έχουν τυχαία πιθανότητα να αγνοηθούν από τον πράκτορα, ενώ η άλλη έχει μη ολισθηρό πάγο (non-slippery ice), όπου οι επιλεγμένες ενέργειες πραγματοποιούνται πάντα από τον πράκτορα. Η παραδοχή αυτή έγινε στον κώδικα όταν φτιάξαμε το περιβάλλον με την βιβλιοθήκη gym. Αν θέλαμε την δεύτερη περίπτωση τότε αρκεί η εξής μικρή μεταποίηση στον κώδικα: `env = gym.make(env_name, is_slippery=False)`.\n","Περισσότερα για το πρόβλημα της Παγωμένης Λίμνης θα αναλυθούν στο επόμενο ερώτημα. \n"],"metadata":{"id":"S_i_7D2f8xbr"}},{"cell_type":"markdown","metadata":{"id":"PAL4we3gDV_J"},"source":["<h2><b>Ερωτήσεις</b></h2>"]},{"cell_type":"markdown","source":["<li>Μελετώντας <a href=\"https://machinelearningjourney.com/index.php/2020/07/02/frozenlake/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>"],"metadata":{"id":"IYNt7wKh9rJz"}},{"cell_type":"markdown","source":["<font color='darkred'>Το πρόβλημα της Παγωμένης Λίμνης (Frozen Lake) αφορά ένα περιβάλλον, το οποίο αποτελεί ένα πλέγμα με “πλακίδια” (tiles). Στην συγκεκριμένη άσκηση το πλέγμα είναι 8x8, δηλαδή διαθέτει 64 tiles, αλλά προφανώς μπορεί να είναι και διαφορετικών διαστάσεων. Στο περιβάλλον αυτό υπάρχει ο πράκτορας (agent), ο οποίος θέλει να μετακινηθεί από το αρχικό πλακίδιο (S: Starting point) στο πλακίδιο στόχο (G: Goal), διασχίζοντας το πλέγμα, δηλαδή την λίμνη, με ασφάλεια. Τα ενδιάμεσα πλακίδια είναι είτε παγωμένη επιφάνεια (F: Frozen surface) είτε τρύπες (H: Hole). Για να διασχίσει λοιπόν με ασφάλεια την λίμνη ο πράκτορας πρέπει να περνάει μόνο από παγωμένη επιφάνεια, η οποία είναι ασφαλής. Αν σε οποιοδήποτε σημείο της διαδρομής πέσει σε τρύπα τότε μένει εκεί για πάντα. Οι κινήσεις που μπορεί να κάνει είναι πάνω, κάτω, δεξιά και αριστερά. Όπως περιέγραψα προηγουμένως στην εκδοχή αυτή το πλακίδιο είναι ολισθηρό, το οποίο σημαίνει ότι ενώ ο πράκτορας μπορεί να θέλει να πάει κάτω, υπάρχει πιθανότητα να «γλιστρήσει» και να πάει για παράδειγμα αριστερά όπου εκεί μπορεί να βρίσκεται μία τρύπα. Στόχος λοιπόν του πράκτορα είναι να μεταβεί από το αρχικό πλακίδιο στο τελικό πλακίδιο χωρίς να πέσει σε κάποια τρύπα. Το πρόβλημα αυτό αποτελεί πρόβλημα βελτιστοποίησης, καθώς η μετάβαση αυτή πρέπει να γίνει με τον μικρότερο αριθμό βημάτων/ δράσεων του πράκτορα. Αξίζει τέλος να σημειωθεί πως μόνο το πλακίδιο στόχος έχει επιβράβευση +1, ενώ όλα τα υπόλοιπα πλακίδια έχουν επιβράβευση 0. "],"metadata":{"id":"SyJxEcedDUZT"}},{"cell_type":"markdown","source":["<li>Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>"],"metadata":{"id":"hkqGddxRJhpf"}},{"cell_type":"markdown","source":["<font color='darkred'>Έστω $(X_n) = (X_0, X_1, X_2, ...)$ μία στοχαστική διαδικασία σε διακριτό χρόνο $n = 0,1,2,...$ και διακριτό χώρο $S$. Λέμε οτι για την $(X_n)$ ισχύει η ιδιότητα Markov όταν για όλους τους χρόνους $n$ και για όλες τις καταστάσεις $(x_0, x_1,...,x_n, x_{n+1})∈S$ ισχύει:\n","$$ P(X_{n+1}=x_{n+1} \\mid X_{n}=x_{n}, X_{n-1} = x_{n-1}, \\dots,X_0=x_0) = P(X_{n+1}=x_{n+1} \\mid X_{n}=x_{n}) $$\n","\n","<font color='darkred'>Η ιδιότητα Markov εκφράζει το γεγονός ότι σε ένα δεδομένο χρονικό βήμα και γνωρίζοντας την τρέχουσα κατάσταση, δεν θα λάβουμε πρόσθετες πληροφορίες για το μέλλον συλλέγοντας πληροφορίες για το παρελθόν. Με άλλα λόγια μας ενδιαφέρει μόνο το παρόν και όχι το παρελθόν.\n","\n","<font color='darkred'>Η ιδιότητα αυτή απλοποιεί το συγκεκριμένο πρόβλημα, καθώς πλέον το πρόβλημα μοντελοποιείται ως MDP (Markov Decision Process) και συνεπώς μπορούν να χρησιμοποιηθούν τεχνικές και αλγόριθμοι δυναμικού προγραμματισμού για την επίλυση του. Η απλοποίηση έγκειται στο γεγονός πως το ευρύτερο πρόβλημα ανάγεται σε πολλά μικρά προβλήματα όπου κάθε φορά μας ενδιαφέρει το επόμενο βήμα του πράκτορα σε σχέση με την θέση που βρίσκεται (στην οποία κατέληξε κάνοντας το προηγούμενο βήμα), χωρίς να μας ενδιαφέρουν τα προηγούμενα βήματα που έκανε, δηλαδή ο τρόπος με τον οποίο κατέληξε στην θέση αυτή."],"metadata":{"id":"ejmPv5cdtlfx"}},{"cell_type":"markdown","source":["<li>Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>"],"metadata":{"id":"Qx2fr4w0JiaB"}},{"cell_type":"markdown","source":["<font color='darkred'>Οι Value Iteration και Policy Iteration είναι αλγόριθμοι οι οποίοι χρησιμοποιούνται για την επίλυση προβλημάτων που είναι MDPs (Markov Decision Processes) υπολογίζοντας την καλύτερη πολιτική $π’(s)$.\n","\n","<font color='darkred'>Ο **Value iteration** χωρίζεται σε δύο βήματα: εύρεση των καλύτερων values (επαναληπτική διαδικασία) και εξαγωγή πολιτικής (μία φορά). Πιο συγκεκριμένα ξεκινάμε με μία αυθαίρετη value function και επαναληπτικά βρίσκουμε τα καλύτερα values. Σε κάθε βήμα τα values μετακινούνται πιο κοντά στα βέλτιστα values. Αν $S$ είναι ο χώρος των καταστάσεων και $A$ ο χώρος των δράσεων τότε κάθε επανάληψη παίρνει $Ο(S^2A)$. Αφού βρούμε τα βέλτιστα values κάνουμε εξαγωγή πολιτικής μία μόνο φορά και με αυτόν τον τρόπο βρίσκουμε την καλύτερη πολιτική. Μαθηματικά ο αλγόριθμος είναι ο εξής:\n","\n","1.  Αρχικοποίησε $V_0(s) = 0$ για όλες τις καταστάσεις $s$\n","2.  Υπολόγισε νέα values για όλες τις καταστάσεις s ως: $$V_{k+1}(s) = \\max_a \\sum_{s'} T(s,a,s')[R(s,a,s')+γV_k(s')]$$\n","1.  Επανέλαβε το βήμα 2 μέχρι να επέλθει σύγκλιση\n","\n","<font color='darkred'>Πρακτικά χρησιμοποιούμε έναν πολύ μικρό αριθμό, έστω $ε$, για να καθορίσουμε πότε έχει συγκλίνει ο αλγόριθμος. Αν η διαφορά των values μεταξύ δύο επαναλήψεων είναι μικρότερη του $ε$ τότε ο αλγόριθμος έχει συγκλίνει. Αφού έχει συγκλίνει εξάγουμε την βέλτιστη πολιτική ως:  $$π'(s) = arg\\max_a \\sum_{s'} T(s,a,s')[R(s,a,s')+γV'(s')]$$\n","\n","<font color='darkred'>Ο **Policy Iteration** χωρίζεται σε δύο βήματα: Policy Evaluation (επαναληπτική διαδικασία) και Policy Improvement (επαναληπτική διαδικασία). Πιο συγκεκριμένα με τον Policy Iteration ξεκινάμε με μια αυθαίρετη πολιτική και την βελτιώνουμε επαναληπτικά μέχρι να βρούμε την βέλτιστή. Στο βήμα Policy Evaluation διορθώνουμε την πολιτική και υπολογίζουμε τις τιμές κάθε κατάστασης (state) στο πλαίσιο της πολιτικής αυτή. Έπειτα για Policy Improvement κάνουμε update την πολιτική χρησιμοποιώντας έναν αλγόριθμο παρόμοιο με τον αλγόριθμο εξαγωγής πολιτικής που αναφέρθηκε προηγουμένως, με την διαφορά ότι ελέγχουμε αν η πολιτική έχει συγκλίνει ή όχι, δηλαδή αν έχει αλλάξει η όχι. Αν η πολιτική έχει αλλάξει τότε επιστρέφουμε στο βήμα Policy Evaluation και συνεχίζουμε την διαδικασία αυτή μέχρι να συγκλίνει η πολιτική, δηλαδή να μην αλλάξει από το πιο πρόσφατο βήμα. Αν $S$ είναι ο χώρος των καταστάσεων και $A$ ο χώρος των δράσεων σε αυτή την περίπτωση κάθε επανάληψη παίρνει $Ο(A^2)$. Μαθηματικά ο αλγόριθμος είναι ο εξής:\n","\n","<font color='darkred'>Για Policy Evaluation:\n","1.  Αρχικοποίησε $V_0(s) = 0$ για όλες τις καταστάσεις $s$\n","2. Υπολόγισε νέα values για όλες τις καταστάσεις s δεδομένης πολιτικής π ως: $$V_{k+1}(s) = \\sum_{s'} T(s,π(s),s')[R(s,π(s),s')+γV_k(s')]$$\n","1.  Επανέλαβε το βήμα 2 μέχρι να επέλθει σύγκλιση\n","\n","<font color='darkred'>Για Policy Improvement:\n","1.  Ενημέρωσε την πολιτική ως: $$π(s) = arg\\max_a \\sum_{s'} T(s,a,s')[R(s,a,s')+γV'(s')]$$\n","2. Επανέλαβε το βήμα 1 μέχρι να επέλθει σύγκλιση\n","\n","<font color='darkred'>Όπως ήδη ανέφερα οι δύο αλγόριθμοι χρησιμοποιούνται για προβλήματα, τα οποία είναι Markov Decision Processes. Συνεπώς είναι εγγυημένο πως και οι δύο θα καταλήξουν σε βέλτιστη πολιτική. Αν η βέλτιστη πολιτική είναι μόνο μία τότε θα καταλήξουν στην ίδια. Αν υπάρχουν περισσότερες από μία βέλτιστες πολιτικές τότε μπορεί να καταλήξουν σε διαφορετική.\n"],"metadata":{"id":"PHdYDdS_nJLj"}},{"cell_type":"markdown","source":["<li>Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n","πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n","Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>"],"metadata":{"id":"HSN4i3Rv9uUs"}},{"cell_type":"markdown","source":["<font color='darkred'>Παρακάτω εκτελώ τον κώδικα της άσκησης για την επίλυση του προβλήματος της Παγωμένης Λίμνης χρησιμοποιώντας τους αλγορίθμους Policy Iteration και Value Iteration. Ο χρόνος εκτέλεσης κάθε προγράμματος υπολογίζεται με την χρήση της `%%time`. "],"metadata":{"id":"a4ePvM3aKeEZ"}},{"cell_type":"markdown","metadata":{"id":"S6mci5P4HJ_1"},"source":["<h2><b>Policy Iteration</b></h2>"]},{"cell_type":"code","metadata":{"id":"_43MjfJ9G8v7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f29258e4-4967-4e89-fd03-e6ac7c9af30d","executionInfo":{"status":"ok","timestamp":1682598352975,"user_tz":-180,"elapsed":4517,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}}},"source":["%%time\n","\"\"\"\n","Solving FrozenLake8x8 environment using Policy iteration.\n","Author : Moustafa Alzantot (malzantot@ucla.edu)\n","\"\"\"\n","\n","import numpy as np\n","import gym\n","from gym import wrappers\n","\n","\n","def run_episode(env, policy, gamma = 1.0, render = False):\n","    \"\"\" Runs an episode and return the total reward \"\"\"\n","    obs = env.reset()\n","    total_reward = 0\n","    step_idx = 0\n","    while True:\n","        if render:\n","            env.render()\n","        obs, reward, done , _ = env.step(int(policy[obs]))\n","        total_reward += (gamma ** step_idx * reward)\n","        step_idx += 1\n","        if done:\n","            break\n","    return total_reward\n","\n","\n","def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n","    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n","    return np.mean(scores)\n","\n","def extract_policy(v, gamma = 1.0):\n","    \"\"\" Extract the policy given a value-function \"\"\"\n","    policy = np.zeros(env.nS)\n","    for s in range(env.nS):\n","        q_sa = np.zeros(env.nA)\n","        for a in range(env.nA):\n","            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n","        policy[s] = np.argmax(q_sa)\n","    return policy\n","\n","def compute_policy_v(env, policy, gamma=1.0):\n","    \"\"\" Iteratively evaluate the value-function under policy.\n","    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n","    and solve them to find the value function.\n","    \"\"\"\n","    v = np.zeros(env.nS)\n","    eps = 1e-10\n","    while True:\n","        prev_v = np.copy(v)\n","        for s in range(env.nS):\n","            policy_a = policy[s]\n","            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n","        if (np.sum((np.fabs(prev_v - v))) <= eps):\n","            # value converged\n","            break\n","    return v\n","\n","def policy_iteration(env, gamma = 1.0):\n","    \"\"\" Policy-Iteration algorithm \"\"\"\n","    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n","    max_iterations = 200000\n","    gamma = 1.0\n","    for i in range(max_iterations):\n","        old_policy_v = compute_policy_v(env, policy, gamma)\n","        new_policy = extract_policy(old_policy_v, gamma)\n","        if (np.all(policy == new_policy)):\n","            print ('Policy-Iteration converged at step %d.' %(i+1))\n","            break\n","        policy = new_policy\n","    return policy\n","\n","\n","if __name__ == '__main__':\n","    env_name  = 'FrozenLake8x8-v0'\n","    env = gym.make(env_name)\n","    env = env.unwrapped\n","    optimal_policy = policy_iteration(env, gamma = 1.0)\n","    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n","    print('Average scores = ', np.mean(scores))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Policy-Iteration converged at step 12.\n","Average scores =  1.0\n","CPU times: user 2.91 s, sys: 185 ms, total: 3.09 s\n","Wall time: 2.89 s\n"]}]},{"cell_type":"markdown","metadata":{"id":"gcikBq6BHRQM"},"source":["<h2><b>Value Iteration</b></h2>"]},{"cell_type":"code","metadata":{"id":"gHvcnTDcHGmH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"64a6ec6e-e024-4f4e-98e9-277afb28ce53","executionInfo":{"status":"ok","timestamp":1682598360354,"user_tz":-180,"elapsed":2522,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}}},"source":["%%time\n","\"\"\"\n","Solving FrozenLake8x8 environment using Value-Itertion.\n","Author : Moustafa Alzantot (malzantot@ucla.edu)\n","\"\"\"\n","import numpy as np\n","import gym\n","from gym import wrappers\n","\n","\n","def run_episode(env, policy, gamma = 1.0, render = False):\n","    \"\"\" Evaluates policy by using it to run an episode and finding its\n","    total reward.\n","    args:\n","    env: gym environment.\n","    policy: the policy to be used.\n","    gamma: discount factor.\n","    render: boolean to turn rendering on/off.\n","    returns:\n","    total reward: real value of the total reward recieved by agent under policy.\n","    \"\"\"\n","    obs = env.reset()\n","    total_reward = 0\n","    step_idx = 0\n","    while True:\n","        if render:\n","            env.render()\n","        obs, reward, done , _ = env.step(int(policy[obs]))\n","        total_reward += (gamma ** step_idx * reward)\n","        step_idx += 1\n","        if done:\n","            break\n","    return total_reward\n","\n","\n","def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n","    \"\"\" Evaluates a policy by running it n times.\n","    returns:\n","    average total reward\n","    \"\"\"\n","    scores = [\n","            run_episode(env, policy, gamma = gamma, render = False)\n","            for _ in range(n)]\n","    return np.mean(scores)\n","\n","def extract_policy(v, gamma = 1.0):\n","    \"\"\" Extract the policy given a value-function \"\"\"\n","    policy = np.zeros(env.nS)\n","    for s in range(env.nS):\n","        q_sa = np.zeros(env.action_space.n)\n","        for a in range(env.action_space.n):\n","            for next_sr in env.P[s][a]:\n","                # next_sr is a tuple of (probability, next state, reward, done)\n","                p, s_, r, _ = next_sr\n","                q_sa[a] += (p * (r + gamma * v[s_]))\n","        policy[s] = np.argmax(q_sa)\n","    return policy\n","\n","\n","def value_iteration(env, gamma = 1.0):\n","    \"\"\" Value-iteration algorithm \"\"\"\n","    v = np.zeros(env.nS)  # initialize value-function\n","    max_iterations = 100000\n","    eps = 1e-20\n","    for i in range(max_iterations):\n","        prev_v = np.copy(v)\n","        for s in range(env.nS):\n","            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n","            v[s] = max(q_sa)\n","        if (np.sum(np.fabs(prev_v - v)) <= eps):\n","            print ('Value-iteration converged at iteration# %d.' %(i+1))\n","            break\n","    return v\n","\n","\n","if __name__ == '__main__':\n","    env_name  = 'FrozenLake8x8-v0'\n","    gamma = 1.0\n","    env = gym.make(env_name)\n","    env = env.unwrapped\n","    optimal_v = value_iteration(env, gamma);\n","    policy = extract_policy(optimal_v, gamma)\n","    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n","    print('Policy average score = ', policy_score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Value-iteration converged at iteration# 2357.\n","Policy average score =  1.0\n","CPU times: user 2.54 s, sys: 212 ms, total: 2.76 s\n","Wall time: 2.57 s\n"]}]},{"cell_type":"markdown","source":["<font color='darkred'>Με βάση τα αποτελέσματα παρατηρούμε πως ο αλγόριθμος Policy Iteration συγκλίνει στην βέλτιστη λύση σε μόλις 12 βήματα, ενώ ο αλγόριθμος Value Iteration σε 2357. Παρόλο που ο αλγόριθμος Value Iteration απαιτεί πολύ περισσότερες επαναλήψεις εωσότου να καταλήξει στην βέλτιστη λύση ο χρόνος εκτέλεσης είναι παρόμοιος και για τους δύο και κινείται στην ίδια τάξη μεγέθους. Συγκεκριμένα ο χρόνος εκτέλεσης του Policy Iteration είναι ελάχιστα μεγαλύτερος (Wall time Policy Iteration: 2.89s, Wall time Value Iteration: 2.57s). Τα αποτελέσματα αυτά συνάδουν με τις διαφορές των δύο αλγορίθμων που αναλύθηκαν παραπάνω, καθώς και τις πολυπλοκότητες τους. Ο Policy Iteration έχει σημαντικά πιο υψηλή πολυπλοκότητα από τον Value Iteration και για αυτό παρόλο που συγκλίνει σε ένα πολύ μικρό αριθμό βημάτων ο χρόνος εκτέλεσης παραμένει παρόμοιος. "],"metadata":{"id":"7SadUl-LtAg_"}}]}