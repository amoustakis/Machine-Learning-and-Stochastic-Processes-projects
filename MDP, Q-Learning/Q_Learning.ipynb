{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Απόστολος Μουστάκης**\n","\n","**AM: 03400182**"],"metadata":{"id":"ey_fp71vl_62"}},{"cell_type":"markdown","source":["<font color='darkred'>Οι απαντήσεις σε όλα τα ερωτήματα γράφονται με αυτό το χρώμα ώστε να ξεχωρίζουν από τις εκφωνήσεις </font>"],"metadata":{"id":"SX1ehJY3mDrd"}},{"cell_type":"markdown","metadata":{"id":"gHFOjD_VY0ld"},"source":["<h1>Q-Learning</h1>\n","\n","<p>Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο Q-Learning, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.</p> <p> Στα πλαίσια του παραδείγματος θα εξετάσετε μία υλοποίηση με Q-learning σχετικά με το σύστημα αυτόματης οδήγησης ενός ταξί. Στα πλαίσια του προβλήματος αυτού θα πρέπει να ικανοποιούνται τα εξής:</p>\n","<ul>\n","<li>Το ταξί θα πρέπει να αφήνει τον πελάτη στη σωστή θέση</li>\n","<li>Το ταξί να ακολουθεί τη συντομότερη δυνατή διαδρομή</li>\n","<li>Να τηρούνται οι κανόνες κυκλοφορίας και ασφάλειας των επιβατών</li>\n","</ul>\n","\n","<p>Στα πλαίσια του προβλήματος έχουμε τα εξής χαρακτηριστικά για την επιβράβευση, τις καταστάσεις και τις ενέργειες.</p> \n","\n","<h4>Επιβράβευση</h4>\n","<ul>\n","<li>Θα έχουμε τη μέγιστη επιβράβευση όταν το ταξί αφήνει έναν πελάτη στην επιθυμητή θέση</li>\n","<li>Θα υπάρχει ποινή στην περίπτωση όπου το ταξί αφήσει τον πελάτη σε κάποιο λανθασμένο σημείο</li>\n","<li>Ο agent θα παίρνει μία μικρή σχετικά ποινή στην περίπτωση όπου αργεί να φτάσει στον τελικό προορισμό</li>\n","</ul>\n","\n","<p>Γενικά οι παραπάνω αρχές συνοψίζονται στα εξής: \"Λαμβάνουμε +20 πόντους για μια επιτυχημένη πτώση και χάνουμε 1 πόντο για κάθε χρονικό βήμα που παίρνει. Υπάρχει επίσης ποινή 10 πόντων για παράνομες ενέργειες παραλαβής και αποχώρησης.\"</p>\n","\n","<h4>Πλήθος Καταστάσεων</h4>\n","<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">\n","<p>Στο παράδειγμά μας έχουμε ένα μικρό χώρο 5*5. Από εκεί και πέρα έχουμε 4 προορισμούς και 5 πιθανές θέσεις του πελάτη (παίρνουμε και την περίπτωση ο πελάτης να είναι ήδη μέσα στο τάξι).</p>\n","<p>Με βάση τα παραπάνω έχουμε 5*5*5*4=500 πιθανές καταστάσεις.</p>\n","\n","<h4>Πλήθος Ενεργειών</h4>\n","<p>Έχουμε έξι ενεργειες για το ταξί, οι οποίες είναι οι εξής:</p>\n","<ul>\n","<li>0=Νότια</li>\n","<li>1=Βόρεια</li>\n","<li>2=Ανατολικά</li>\n","<li>3=Δυτικά</li>\n","<li>4=Επιβίβαση</li>\n","<li>5=Αποβίβαση</li>\n","</ul>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HOQpQ0DygDzt"},"source":["<p>Πριν συνεχίσετε στην άσκηση να απαντήσετε στο εξής ερώτημα:</p>\n","\n","<b><p>1. Να περιγράψετε σύντομα τον αλγόριθμο Q-Learning. Σε ποια προβλήματα θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η Ενισχυτική Μάθηση (Reinforcement Learning); Ποια είναι η βασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration;</p></b>"]},{"cell_type":"markdown","source":["\n","<font color='darkred'>Η βασική ιδέα του Q-learning είναι ότι υπολογίζουμε την μελλοντική αναμενόμενη επιβράβευση για κάθε δράση σε κάθε κατάσταση. Αυτό ονομάζεται Q-value (ένα ζεύγος κατάσταση-δράση). Με αυτόν τον τρόπο μπορούμε γρήγορα και εύκολα να ανακτήσουμε μία πολιτική: σε μία κατάσταση s απλά επιλέγουμε την δράση με το μεγαλύτερο Q-value.\n","$$π'(s) = arg\\max_a Q(s,a)$$\n","Ο αλγόριθμος Q-learning μοντελοποιείται μαθηματικά ως εξής:\n","1.   Αρχικοποίησε $Q(s,a)=0$ για όλα τα ζευγάρια κατάσταση-δράση\n","2.   Στην κατάσταση $s$ κάνε την δράση $α=π’(s)$, δηλαδή την καλύτερη δράση, ώστε να καταλήξεις σε κατάσταση $s’$ με επιβράβευση $r$ \n","3.\tΥπολόγισε την νέα εκτιμώμενη Q-value:\n","$$Q'(s,a) = r + γ \\max_{α'} Q_k(s',a') $$\n","2.   Ενημερώστε την τρέχουσα Q-value εξομαλύνοντας τη νέα εκτιμώμενη και την υπάρχουσα:\n","$$Q_{k+1}(s,a) = (1-a)Q_k(s,a) + a Q'(s,a) $$\n","1.   Πήγαινε στο βήμα 2 μέχρι να επέλθει σύγκλιση\n","\n","<font color='darkred'>Η Ενισχυτική Μάθηση (Reinforcement Learning) ταιριάζει ως τρόπος εκμάθησης σε προβλήματα όπου έχουν σαν στόχο να εκπαιδευτεί ένας αλγόριθμος που θα προσαρμόζει την συμπεριφορά του σε μία συγκεκριμένη κατάσταση. Ο αλγόριθμος αυτός εξερευνάει το περιβάλλον παίρνοντας αποφάσεις και αποκτώντας εμπειρία μέσω της ανατροφοδότησης του με αυτό, η οποία δίνεται με την μορφή επιβραβεύσεων και κυρώσεων. Χαρακτηριστικό παράδειγμα εφαρμογής της Ενισχυτικής Μάθησης είναι σε παιχνίδια, όπως το σκάκι και το Go, όπου ο αλγόριθμος κερδίζει τους καλύτερους παίκτες στον κόσμο.\n","\n","<font color='darkred'>Η κύρια διαφορά του Q-Learning από τους αλγορίθμους Policy και Value Iteration είναι ότι ο πράκτορας δεν γνωρίζει τις πιθανότητες μετάβασης των καταστάσεων και τις ανταμοιβές. Ο πράκτορας ανακαλύπτει ότι υπάρχει μία ανταμοιβή για την μετάβαση από μία κατάσταση σε μία άλλη μέσω μίας δεδομένης δράσης μόνο όταν κάνει την συγκεκριμένη δράση και λάβει την ανταμοιβή. Ομοίως, υπολογίζει μόνο ποιες μεταβάσεις είναι διαθέσιμες από μία δεδομένη κατάσταση όταν καταλήγει σε αυτή την κατάσταση και εξετάζει τις επιλογές του. Ένα οι καταστάσεις είναι στοχαστικές μαθαίνει την πιθανότητα μετάβασης μεταξύ των καταστάσεων παρατηρώντας πόσο συχνά συμβαίνουν διαφορετικές μεταβάσεις. Ο αλγόριθμος Q-Learning είναι model free: Δεν πραγματοποιεί υποθέσεις για το περιβάλλον αλλά το εξερευνάει. \n","\n","\n","\n","\n"],"metadata":{"id":"0Nk9HGq2Q8w_"}},{"cell_type":"markdown","metadata":{"id":"pc11bCDZgfSx"},"source":["<p>Στη συνέχεια θα πρέπει να φορτώσετε τη βιβλιοθήκη gym καθώς και το σχετικό dataset<p>"]},{"cell_type":"code","metadata":{"id":"y_95lIAzWYpp"},"source":["!pip uninstall -y gym\n","!pip install setuptools==66\n","!pip install gym==0.18.0\n","!pip install gym[atari]\n","!pip install cmake scipy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDHLb5PGWdwr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682811843426,"user_tz":-180,"elapsed":2231,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}},"outputId":"ad7b8e26-1a3f-41ab-87d1-801386a67d34"},"source":["import gym\n","\n","env = gym.make(\"Taxi-v3\").env\n","\n","env.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+\n","|R: | : :\u001b[34;1mG\u001b[0m|\n","| : | : :\u001b[43m \u001b[0m|\n","| : : : : |\n","| | : | : |\n","|\u001b[35mY\u001b[0m| : |B: |\n","+---------+\n","\n"]}]},{"cell_type":"code","metadata":{"id":"396kTtOrW-cO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682811851238,"user_tz":-180,"elapsed":281,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}},"outputId":"a2c8031c-7d91-445b-e918-59307bacf27f"},"source":["env.reset() # reset environment to a new, random state\n","env.render()\n","\n","print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+\n","|R: | : :G|\n","| : |\u001b[43m \u001b[0m: : |\n","| : : : : |\n","| | : | : |\n","|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n","+---------+\n","\n","Action Space Discrete(6)\n","State Space Discrete(500)\n"]}]},{"cell_type":"markdown","metadata":{"id":"-UC6-XuIhF5_"},"source":["<p>Παρακάτω ορίζουμε τις συνεταγμένες του ταξί, τη θέση του πελάτη και το σημείο προορισμού</p>"]},{"cell_type":"code","metadata":{"id":"nPSOw5CdXFx1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682811853424,"user_tz":-180,"elapsed":290,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}},"outputId":"45d9282a-b2a2-4141-9236-c0cb73626ed7"},"source":["state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n","print(\"State:\", state)\n","\n","env.s = state\n","env.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["State: 328\n","+---------+\n","|\u001b[35mR\u001b[0m: | : :G|\n","| : | : : |\n","| : : : : |\n","| |\u001b[43m \u001b[0m: | : |\n","|\u001b[34;1mY\u001b[0m| : |B: |\n","+---------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"NsAkQJhchVFy"},"source":["<p>Παρακάτω είναι η μήτρα επιβράβευσης για το state που ορίσαμε στο προηγούμενο βήμα</p>"]},{"cell_type":"code","metadata":{"id":"j7oDbznIXOJo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682811855478,"user_tz":-180,"elapsed":368,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}},"outputId":"3b06d3eb-bdf2-4d85-f95a-9be52b2e8796"},"source":["env.P[328]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: [(1.0, 428, -1, False)],\n"," 1: [(1.0, 228, -1, False)],\n"," 2: [(1.0, 348, -1, False)],\n"," 3: [(1.0, 328, -1, False)],\n"," 4: [(1.0, 328, -10, False)],\n"," 5: [(1.0, 328, -10, False)]}"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"CtROeD1ph6kk"},"source":["<p> Τρέχουμε το παράδειγμά μας χωρις τη χρήση Q-Learning.</p>\n","\n","<b><p>2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;</p></b>"]},{"cell_type":"markdown","source":["<font color='darkred'>Προφανώς τα αποτελέσματα δεν είναι καθόλου ικανοποιητικά. Ο πράκτορας κάνει πάρα πολλά βήματα και ένα μεγάλο πλήθος από αυτά αντιστοιχεί σε σφάλματα (penalties). Συγκεκριμένα κάθε φορά περίπου το 1/3 των βημάτων αντιστοιχεί σε σφάλματα. Αυτό συμβαίνει επειδή ο πράκτορας δεν έχει την δυνατότητα να μάθει από την εμπειρία του παρελθόντος και να την αξιοποιήσει ώστε να παίρνει καλύτερες αποφάσεις. Όσες φορές και να τρέξουμε τον αλγόριθμο δεν θα βελτιστοποιηθεί ποτέ. Με την χρήση του Q-learning ο πράκτορας θα χρησιμοποιήσει τις ανταμοιβές του περιβάλλοντος για να μάθει με την πάροδο του χρόνου την καλύτερη ενέργεια που πρέπει να κάνει σε μία δεδομένη κατάσταση. Έτσι θα έχουμε πολύ καλύτερα αποτελέσματα."],"metadata":{"id":"WDfY0jk89Za-"}},{"cell_type":"code","metadata":{"id":"vKHhcDxVXUFz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682811857550,"user_tz":-180,"elapsed":330,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}},"outputId":"93f1c913-2b30-4974-d42a-f2451e6cbbb2"},"source":["env.s = 328  # set environment to illustration's state\n","\n","epochs = 0\n","penalties, reward = 0, 0\n","\n","frames = [] # for animation\n","\n","done = False\n","\n","while not done:\n","    action = env.action_space.sample()\n","    state, reward, done, info = env.step(action)\n","\n","    if reward == -10:\n","        penalties += 1\n","    \n","    # Put each rendered frame into dict for animation\n","    frames.append({\n","        'frame': env.render(mode='ansi'),\n","        'state': state,\n","        'action': action,\n","        'reward': reward\n","        }\n","    )\n","\n","    epochs += 1\n","    \n","    \n","print(\"Timesteps taken: {}\".format(epochs))\n","print(\"Penalties incurred: {}\".format(penalties))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Timesteps taken: 960\n","Penalties incurred: 344\n"]}]},{"cell_type":"markdown","metadata":{"id":"aj3s09rsizVm"},"source":["<p>Τώρα θα προσπαθήσουμε να λύσουμε το πρόβλημά μας με τη χρήση του Q-Learning.</p>\n","\n","<b><p>3. Τι γνωρίζετε για τις παραμέτρους α και γ. Τι θα συμβεί αν έχουν τιμές ίσες με 1;</p></b>"]},{"cell_type":"markdown","source":["<font color='darkred'>Η παράμετρος $γ$ αφορά τον παράγοντα έκπτωσης (discount factor) και καθορίζει πόση σημασία δίνεται σε μελλοντικές επιβραβεύσεις. Ισχύει $0 \\leq γ \\leq 1$. Μία υψηλή τιμή του $γ$, κοντά στο 1, δηλώνει πως δίνεται μεγάλη βαρύτητα σε μελλοντικές επιβραβεύσεις, ενώ μία χαμηλή τιμή του $γ$, κοντά στο 0, σημαίνει πως ο πράκτορας δίνει περισσότερη σημασία στις άμεσες επιβραβεύσεις και συνεπώς ο αλγόριθμος γίνεται πιο άπληστος. Στην ακραία περίπτωση όπου $γ = 1$ ο αλγόριθμος δεν συγκλίνει, καθώς η value function αποκλίνει.\n","\n","<font color='darkred'>Η παράμετρος $α$ είναι ο ρυθμός εκμάθησης, ο οποίος αφορά τον βαθμό στον οποίο οι Q-values ενημερώνονται σε κάθε επανάληψη. Ισχύει $0 \\leq α \\leq 1$.  Στην περίπτωση όπου $α = 0$ ο πράκτορας δεν μαθαίνει καθόλου από το περιβάλλον, καθώς οι Q-values δεν ανανεώνονται. Από την άλλη, όσο το $α$ πλησιάζει το 1 ο αλγόριθμος μαθαίνει πολύ γρήγορα. Στην ακραία περίπτωση όπου $α = 1$ οι νέες Q-values θα παρακάμψουν εντελώς τις παλιές και συνεπώς ο αλγόριθμος δεν κρατάει μνήμη. Αυτό μπορεί να οδηγήσει σε Q-values οι οποίες είναι ασταθείς και ταλαντεύονται μεταξύ τιμών εμποδίζοντας τον αλγόριθμο στο να συγκλίνει σε βέλτιστη λύση.\n"],"metadata":{"id":"iRx8GkjyJkot"}},{"cell_type":"code","metadata":{"id":"JJk3NTfcXrrA"},"source":["import numpy as np\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oy2Yg8DTXtHW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682811936821,"user_tz":-180,"elapsed":67698,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}},"outputId":"1692c5f9-2f88-419c-a7f4-6a3e81d1f383"},"source":["%%time\n","\"\"\"Training the agent\"\"\"\n","\n","import random\n","from IPython.display import clear_output\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","for i in range(1, 100001):\n","    state = env.reset()\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        next_state, reward, done, info = env.step(action) \n","        \n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","        \n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 100000\n","Training finished.\n","\n","CPU times: user 1min 2s, sys: 7.83 s, total: 1min 10s\n","Wall time: 1min 7s\n"]}]},{"cell_type":"code","metadata":{"id":"dxt4fmvGYBOm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682798822209,"user_tz":-180,"elapsed":317,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}},"outputId":"efb3d4b4-1f1b-4889-fb5f-b2d0f5af2fd5"},"source":["q_table[328]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ -2.41505176,  -2.27325184,  -2.40231418,  -2.35947929,\n","        -9.77058538, -10.20595373])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"6W9JE9yOYGgP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682813396559,"user_tz":-180,"elapsed":266,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}},"outputId":"035cb95d-3158-40d6-986a-f2d595b10cae"},"source":["\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 100\n","rewards=[]\n","\n","for _ in range(episodes):\n","    state = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","        rewards.append(reward)\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","rewards = np.asarray(rewards)\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")\n","print(f\"Average reward per step: {rewards.mean():.2f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Results after 100 episodes:\n","Average timesteps per episode: 12.98\n","Average penalties per episode: 0.0\n","Average reward per step: 0.62\n"]}]},{"cell_type":"markdown","metadata":{"id":"5R7gW1nLj-qE"},"source":["<b><p>4. Συγκρίνετε τους δύο αλγορίθμους με βάση τις παρακάτω μετρικές</p>\n","<ul>\n","<li>Μέσος αριθμός παραβάσεων ανά επεισόδιο</li>\n","<li>Μέσος αριθμός βημάτων ανά διαδρομή</li>\n","<li>Μέσος αριθμός ανταμοιβών ανά κίνηση</li>\n","</ul>\n","<p>Τις παραπάνω συγκρίσεις να τις κάνετε για 100 επεισόδια.</p>\n","</b>"]},{"cell_type":"code","source":["\"\"\"Evaluate agent's performance before Q-learning\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 100\n","rewards=[]\n","\n","for i in range(episodes):\n","  env.s = 328  # set environment to illustration's state\n","\n","  epochs = 0\n","  penalties, reward = 0, 0\n","\n","  frames = [] # for animation\n","\n","  done = False\n","\n","  while not done:\n","      action = env.action_space.sample()\n","      state, reward, done, info = env.step(action)\n","      rewards.append(reward)\n","\n","      if reward == -10:\n","         penalties += 1\n","    \n","      #Put each rendered frame into dict for animation\n","      frames.append({\n","          'frame': env.render(mode='ansi'),\n","          'state': state,\n","          'action': action,\n","          'reward': reward\n","          }\n","      )\n","\n","      epochs += 1\n","\n","  total_penalties += penalties\n","  total_epochs += epochs\n","\n","rewards = np.asarray(rewards)\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")\n","print(f\"Average reward per step: {rewards.mean():.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bgZciuDVg_h1","executionInfo":{"status":"ok","timestamp":1682814085665,"user_tz":-180,"elapsed":13064,"user":{"displayName":"Αποστόλης Μουστάκης","userId":"15849191241211199691"}},"outputId":"c4de8370-0f67-4278-b8b4-a5ccb84e3e75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Results after 100 episodes:\n","Average timesteps per episode: 1837.63\n","Average penalties per episode: 596.39\n","Average reward per step: -3.91\n"]}]},{"cell_type":"markdown","source":["<font color='darkred'>Η σύγκριση των αλγορίθμων γίνεται με τις παρακάτω μετρικές:\n","*  Μέσος αριθμός παραβάσεων ανά επεισόδιο: Όσο μικρότερος είναι ο αριθμός αυτός τόσο καλύτερη είναι η απόδοση του agent. Ιδανικά θα θέλαμε η μέτρηση αυτή να είναι 0 ή πολύ κοντά στο 0.\n","*  Μέσος αριθμός βημάτων ανά διαδρομή: Επίσης θέλουμε αυτός ο αριθμός να είναι μικρός, καθώς θέλουμε ο agent να κάνει τα ελάχιστα δυνατά βήματα για να φτάσει στον προορισμό (shortest path).\n","*  Μέσος αριθμός ανταμοιβών ανά κίνηση: Όσο μεγαλύτερη είναι η ανταμοιβή σημαίνει ότι ο agent κάνει το σωστό. Στην συγκεκριμένη περίπτωση, όπου τα timesteps και τα penalties ανταμείβονται αρνητικά, μια υψηλότερη μέση ανταμοιβή θα σήμαινε ότι ο agent φτάνει στον προορισμό του όσον τον δυνατόν γρηγορότερα και με τις λιγότερες κυρώσεις. </font>\n","\n","<font color='darkred'>Τα συγκεντρωτικά αποτελέσματα των δύο αλγορίθμων για 100 επεισόδια φαίνονται στον παρακάτω πίνακα. Είναι εμφανές ότι ο αλγόριθμος Q-Learning έχει καταπληκτικά αποτελέσματα.\n","<center>\n","\n","||  Χωρίς Q-Learning (Random)| Με Q-Learning | \n","| :----:       |    :----:   |   :----:   |     \n","| Μέσος αριθμός παραβάσεων ανά επεισόδιο|596.39|0.0\n","| Μέσος αριθμός βημάτων ανά διαδρομή|1837.63|12.98\n","| Μέσος αριθμός ανταμοιβών ανά κίνηση|-3.98|0.62\n","\n","</center> \n"],"metadata":{"id":"Vxbm2svIc3Ax"}}]}